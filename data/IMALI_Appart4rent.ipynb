{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "393af9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #to send HTTP requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Modules to connect with postgress\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import csv\n",
    "\n",
    "# Modules to run automatically\n",
    "import schedule # To handle scheduling tasks.\n",
    "import time as tm # For time-related operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46eb50",
   "metadata": {},
   "source": [
    "### Links Extraction, Data Collection, Data Cleaning and saving to Postgres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_count = 0  # Counter for tracking the number of times the script has run\n",
    "def scrape_and_clean_data_appart4rent():\n",
    "    global run_count\n",
    "    run_count += 1\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    \n",
    "    #function to extract links\n",
    "    #.........................\n",
    "    def get_real_estate_data(filename):\n",
    "        # Empty list to store links\n",
    "        Links = []\n",
    "        #Looping through the pages\n",
    "        for page in range(1,241):\n",
    "            # Headers used to specify how the HTTP request should be processed by the server. \n",
    "            # i.e - User-Agent header specifying the user agent string that should be sent with the request,\n",
    "            # - Accept header specifying the types of content that the client can handle.\n",
    "            headers = {'Accept-Encoding': 'gzip, deflate, sdch','Accept-Language': 'en-US,en;q=0.8','Upgrade-Insecure-Requests': '1',\n",
    "                       'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "                       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                       'Cache-Control': 'max-age=0','Connection': 'keep-alive',}\n",
    "            # Requesting desired URL\n",
    "            try:\n",
    "                response_text = requests.get(filename + str(page), headers=headers, allow_redirects=False).text\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response_text, 'html.parser')\n",
    "\n",
    "                # Loop through every house's link\n",
    "                for div in soup.findAll('div', {'class': 'item-list'}):\n",
    "                    # Find the link for the current apartment\n",
    "                    link = div.find('a').attrs['href']\n",
    "                    # Keep the links in the Links list\n",
    "                    Links.append(\"https://imali.biz\" + link)  \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"An error occurred while fetching data: {e}\")\n",
    "        # Create a DataFrame from the list\n",
    "        imali_data = pd.DataFrame({'IMALI-appartment4rent_links': Links})\n",
    "        # Saving the DataFrame to a CSV file\n",
    "        imali_data.to_csv(\"IMALI_Apartment4rent_links.csv\")\n",
    "    \n",
    "    \n",
    "     # Function to extract apartment for sale data\n",
    "    #............................................\n",
    "    def extract_appart4rent_data():\n",
    "        apartment_for_Rent_links = pd.read_csv('IMALI_Apartment4Rent_links.csv', usecols=[1])\n",
    "        # List to store extracted data from all the houses for sale\n",
    "        all_appart_list = []\n",
    "        # Iterating through the first 2000 links in the dataframe\n",
    "        for i in apartment_for_Rent_links['IMALI-appartment4rent_links']:\n",
    "            headers = {'accept': 'application/json',}\n",
    "            # Requesting desired URL\n",
    "            response_text = requests.get(i,headers=headers, allow_redirects=False).text \n",
    "            # Parsing the response text\n",
    "            soup = BeautifulSoup(response_text, 'html.parser')\n",
    "\n",
    "            # List to keep extracted data for each appartment\n",
    "            each_appart_list = []\n",
    "            # Iterating through each div with class \"media-body\"\n",
    "            for div in soup.findAll('div', {'class': \"media-body\"}):\n",
    "                # Find the specifications of the appartment\n",
    "                specifications = div.find('span', {'class': \"media-heading\"}).text.strip()\n",
    "                # Keep the specifications of the appartment\n",
    "                each_appart_list.append(specifications)\n",
    "                \n",
    "            # Loop through each div with class \"col-md-4\" (Price & Location)\n",
    "            for i in soup.findAll('div', {'class':\"col-md-4\"}):\n",
    "                # Find the price of the appartment\n",
    "                price = i.find('p').text.replace(\"\\t\",\"\").strip()\n",
    "                # Keep the price of the appartment\n",
    "                each_appart_list.append(price)\n",
    "                # Find the district of the appartment\n",
    "                district = i.findAll('p')\n",
    "                # Keep the district name of the appartment\n",
    "                each_appart_list.append(district)\n",
    "                # Find the sector of the appartment\n",
    "                sector = i.findAll('p')\n",
    "                # Keep the sector name of the appartment\n",
    "                each_appart_list.append(sector)\n",
    "\n",
    "            # Loop through each span with class \"date\" (Date) & to find date posted\n",
    "            for div in soup.findAll('span', {\"class\":\"date\"}):\n",
    "                Date = div\n",
    "                # Keep the date posted\n",
    "                each_appart_list.append(Date.text.strip())\n",
    "            # Keep details of all appartments     \n",
    "            all_appart_list.append(each_appart_list)\n",
    "\n",
    "        #Create dataframe to keep appartments details\n",
    "        columns = [\"Ref_number\",\"Build_Year\",\"Floors\",\"Sitting_Rooms\",\"Dining_Rooms\",\"Bedrooms\",\"Wardrobes\",\"Bathrooms\",\"Car_Parking\",\"Ancillary\",\"Plot_number\",\"LandSize\", \"Price\", \"District\",\"Sector\",\"Date\"]\n",
    "        all_appart_df = pd.DataFrame(all_appart_list, columns = columns) \n",
    "        # Save the dataframe to csv file\n",
    "        all_appart_df.to_csv(\"IMALI_appart4rent_Details.csv\") \n",
    "        \n",
    "\n",
    "    # Function to clean and save the apartment for sale data\n",
    "    #........................................................\n",
    "    def clean_and_save_appart4rent_data():\n",
    "        appartment4rent = pd.read_csv(\"IMALI_appart4rent_Details.csv\")\n",
    "        # Set the 'Date' column as the index & convert it to datetime format\n",
    "        appartment4rent = appartment4rent.set_index(pd.to_datetime(appartment4rent['Date']))\n",
    "        # Drop unwanted cols from the df\n",
    "        appartment4rent = appartment4rent.drop(['Ref_number', 'Plot_number', \"Unnamed: 0\", \"Date\"], axis =1)\n",
    "\n",
    "        # Clean respective columns accordingy\n",
    "        appartment4rent['LandSize'] = [int(area.replace(\"m2\",\"\").replace(\",\",\"\")) for area in appartment4rent['LandSize']]\n",
    "        appartment4rent['District'] = appartment4rent['District'].str.split('District:</strong>').str[1].str.split('<').str[0]\n",
    "        appartment4rent['Sector'] = appartment4rent['Sector'].str.split('Sector:</strong>').str[1].str.split('<').str[0]\n",
    "        appartment4rent['Price'] = appartment4rent['Price'].str.replace(r'[^0-9]', '', regex=True)\n",
    "\n",
    "        # Save the clean df to a CSV file named \"IMALI_House4rent_Data.csv\"\n",
    "        appartment4rent.to_csv(\"IMALI_apartment4rent_Clean_data.csv\")\n",
    "\n",
    "    \n",
    "    # Function to create BD to postgres\n",
    "    #...................................\n",
    "    def infer_column_types(csv_file_path):\n",
    "        # Read data from CSV file and infer column types\n",
    "        with open(csv_file_path, 'r') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            header = next(reader)  # Get the header row\n",
    "            data_sample = next(reader)  # Get a sample row of data\n",
    "\n",
    "        column_types = []\n",
    "        for value in data_sample:\n",
    "            # Check for specific data types\n",
    "            if value.isdigit():\n",
    "                if int(value) > 2147483647:  # Check for big integer values\n",
    "                    column_types.append('BIGINT')\n",
    "#                 else:\n",
    "#                     column_types.append('INTEGER')\n",
    "            elif value.replace('.', '', 1).isdigit():\n",
    "                column_types.append('DECIMAL')\n",
    "            elif value.strip():\n",
    "                try:\n",
    "                    datetime.strptime(value, '%Y-%m-%d')  # Check for date values\n",
    "                    column_types.append('DATE')\n",
    "                except ValueError:\n",
    "                    column_types.append('VARCHAR(100)')\n",
    "            else:\n",
    "                column_types.append('VARCHAR(100)')\n",
    "\n",
    "        return header, column_types\n",
    "\n",
    "\n",
    "    def insert_data_from_csv_to_imali_table(csv_file_path):\n",
    "        # Establish a connection to the PostgreSQL database\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"postgres\",\n",
    "            password=\"Inyange\"\n",
    "        )\n",
    "\n",
    "        # Create a cursor object to interact with the database\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Create the database if it doesn't exist\n",
    "        cur.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname='IMALI_Properties'\")\n",
    "        database_exists = cur.fetchone()\n",
    "\n",
    "        if not database_exists:\n",
    "            cur.execute(\"CREATE DATABASE IMALI_Properties\")\n",
    "            conn.commit()\n",
    "            print(\"Database 'IMALI_Properties' created successfully.\")\n",
    "\n",
    "        # Close the cursor and current connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        # Connect to the Trial database\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            database=\"IMALI_Properties\",\n",
    "            user=\"postgres\",\n",
    "            password=\"Inyange\"\n",
    "        )\n",
    "\n",
    "        # Create a new cursor object to interact with the Trial database\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Check if the table exists\n",
    "        cur.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'apartment_for_rent')\")\n",
    "        table_exists = cur.fetchone()[0]\n",
    "\n",
    "        # Create the table if it doesn't exist\n",
    "        if not table_exists:\n",
    "            header, column_types = infer_column_types(csv_file_path)\n",
    "\n",
    "            # Construct the CREATE TABLE query\n",
    "            create_table_query = 'CREATE TABLE apartment_for_rent ('\n",
    "            for column_name, column_type in zip(header, column_types):\n",
    "                create_table_query += f'{column_name} {column_type}, '\n",
    "            create_table_query = create_table_query.rstrip(', ') + ')'\n",
    "    \n",
    "            cur.execute(create_table_query)\n",
    "            conn.commit()\n",
    "            print(\"Table 'apartment_for_rent' created successfully.\")\n",
    "        else:\n",
    "            # Update the table if it exists\n",
    "            update_table_query = '''\n",
    "                ALTER TABLE apartment_for_rent\n",
    "                ADD COLUMN IF NOT EXISTS department VARCHAR(100)\n",
    "            '''\n",
    "            cur.execute(update_table_query)\n",
    "            conn.commit()\n",
    "            print(\"Table 'apartment_for_rent' updated successfully.\")\n",
    "\n",
    "        # Read data from CSV file\n",
    "        with open(csv_file_path, 'r') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            next(reader)  # Skip header row\n",
    "            imali_data = list(reader)\n",
    "        # Sort the data by date\n",
    "        sorted_data = sorted(imali_data, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\n",
    "        # Insert new rows into the table if they don't already exist\n",
    "        insert_query = '''\n",
    "            INSERT INTO apartment_for_rent (Date, Build_Year, Floors, Sitting_Rooms, Dining_Rooms, Bedrooms, Wardrobes, Bathrooms, Car_Parking, Ancillary, LandSize, Price, District, Sector)\n",
    "            SELECT %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM apartment_for_rent WHERE Date = %s AND Build_Year = %s AND Floors = %s AND Sitting_Rooms = %s\n",
    "                AND Dining_Rooms = %s AND Bedrooms = %s AND Wardrobes = %s AND Bathrooms = %s\n",
    "                AND Car_Parking = %s AND Ancillary = %s AND LandSize = %s AND Price = %s\n",
    "                AND District = %s AND Sector = %s\n",
    "            )\n",
    "        '''\n",
    "        data_inserted = False  # Flag variable to track new row insertion\n",
    "        for data in sorted_data:\n",
    "            cur.execute(insert_query, [*data, *data])\n",
    "            if cur.rowcount > 0:\n",
    "                conn.commit()\n",
    "                if not data_inserted:\n",
    "                    print(\"New row(s) inserted:\")\n",
    "                    data_inserted = True\n",
    "                print(data)\n",
    "        if not data_inserted:\n",
    "            print(\"No new data was inserted.\")\n",
    "\n",
    "\n",
    "        # Close the cursor and connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    \n",
    "     \n",
    "    # Function to combine all the functions into one\n",
    "    #...............................................\n",
    "    def combined_function():\n",
    "        # URL of the real estate listings page on 'imali.biz', with the page number left blank\n",
    "        filename = 'https://imali.biz/category/0/91/search?pg='\n",
    "        imali_data = get_real_estate_data(filename)\n",
    "        extract_appart4rent_data()\n",
    "        clean_and_save_appart4rent_data()  \n",
    "        # Specify the path to the CSV file\n",
    "        csv_file_path = csv_file_path = 'C:\\\\Users\\\\yinyange\\\\Documents\\\\BNR\\\\Imali_properties\\\\House&Appart(all)\\\\Appart4rent(all)\\\\IMALI_apartment4rent_Clean_data.csv'\n",
    "        # Insert data from the CSV file to the table\n",
    "        insert_data_from_csv_to_imali_table(csv_file_path)\n",
    "\n",
    "    # Call the combined function\n",
    "    combined_function()\n",
    "    print(f\"Script has run {run_count} times. Current time: {current_time}\\n\\n\")\n",
    "scrape_and_clean_data_appart4rent()   \n",
    "\n",
    "    \n",
    "# Automation\n",
    "# ..........\n",
    "# ..........\n",
    "schedule.every(5).minutes.do(scrape_and_clean_data_appart4rent)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    tm.sleep(30) # Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b9ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
